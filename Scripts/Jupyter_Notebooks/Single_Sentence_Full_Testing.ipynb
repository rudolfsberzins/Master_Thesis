{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sys import argv\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import glob\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_entities(ents_name):\n",
    "\n",
    "\tents = pd.read_csv(ents_name, sep = '\\t', names = ['Given_ID', 'UNUSED', 'Real_ID'])\n",
    "\n",
    "\treturn ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_nec_pairs(pairs_name):\n",
    "    \n",
    "    nec_pairs = pd.read_csv(pairs_name, sep = '\\t', names = [\"Entity_name_x\",\"SerialNo_x\", \"Entity_name_y\", \"SerialNo_y\", \"Text\"])\n",
    "    \n",
    "    return nec_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_interactions(interactions_name):\n",
    "\t\"\"\"Opens a TSV File Containing the Interactions from STRING DB\n",
    "\n",
    "\tINPUT: Name of the tsv file\n",
    "\n",
    "\tOUTPUT: Pandas DataFrame of the .tsv file\"\"\"\n",
    "\t\n",
    "\tinteractions_file = pd.read_csv(interactions_name, sep = '\\t')\n",
    "\n",
    "\treturn interactions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ents = \"./yeast/yeast_entities.tsv\"\n",
    "ints = \"./4932.protein.actions.v10.txt/4932.protein.actions.v10.txt\"\n",
    "pairs_nec = open_nec_pairs(\"./Results/Pairs_With_Sentences_Only_nec.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_pre_bow(pairs_nec, ents):\n",
    "\n",
    "\tprint \"Producing Pre BOW data frame\"\n",
    "\n",
    "\tentities = open_entities(ents)\n",
    "\tpairs_copy = pairs_nec.copy()\n",
    "\tfor i,r in pairs_copy.iterrows():\n",
    "\n",
    "\t\t#Add Real_ID's\n",
    "\t\tif r[\"SerialNo_x\"] in set(entities[\"Given_ID\"].tolist()) and r[\"SerialNo_y\"] in set(entities[\"Given_ID\"].tolist()):\n",
    "\t\t\tx = entities.loc[entities[\"Given_ID\"] == r[\"SerialNo_x\"], (\"Real_ID\")]\n",
    "\t\t\ty = entities.loc[entities[\"Given_ID\"] == r[\"SerialNo_y\"], (\"Real_ID\")]\n",
    "\t\t\tpairs_copy.set_value(i, \"Real_ID_x\", x.values[0])\n",
    "\t\t\tpairs_copy.set_value(i, \"Real_ID_y\", y.values[0])\n",
    "\n",
    "\t\t#Mask entity names in text\n",
    "\t\ttry:\n",
    "\t\t\tsub1 = re.sub(r[\"Entity_name_x\"], '', r[\"Text\"])\n",
    "\t\t\tsub2 = re.sub(r[\"Entity_name_y\"], '', sub1)\n",
    "\t\texcept re.error:\n",
    "\t\t\tprint \"Couldn't mask \", r[\"Entity_name_y\"], \" with RegEx\\n\", \"Trying a different way!\"\n",
    "\t\t\tsub2 = sub1.replace(r[\"Entity_name_y\"], \"\")\n",
    "\t\t\tpairs_copy.set_value(i, \"Text\", sub2)\n",
    "\n",
    "\t\tpairs_copy.set_value(i, \"Text\", sub2)\n",
    "\n",
    "\tbow_df = pairs_copy[[\"Real_ID_x\", \"Real_ID_y\", \"Text\"]] #Get only Real_IDs and Texts\n",
    "\t# grouped = bow_df.groupby([\"Real_ID_x\", \"Real_ID_y\"]) #Group by Real_ID's\n",
    "\t# grouped_text = grouped['Text'].apply(lambda x: ' '.join(x.astype(str))).reset_index() #Concat the text files\n",
    "\n",
    "\t# #Initialize some necessary vars\n",
    "\t# col_names = [\"Real_ID_x\", \"Real_ID_y\", \"Text\"]\n",
    "\t# temp_df = pd.DataFrame(columns = col_names)\n",
    "\t# exclude = []\n",
    "\n",
    "\t# # Catch semi-duplicate pairs\n",
    "\t# for i,r in grouped_text.iterrows():\n",
    "\t# \tpair = [r[\"Real_ID_x\"], r[\"Real_ID_y\"]]\n",
    "\t# \t#Check if there is a reverse version of the pair\n",
    "\t# \tif grouped_text.query('@pair[0] == Real_ID_y and @pair[1] == Real_ID_x').empty: \n",
    "\t# \t\tpass\n",
    "\t# \telif r.name not in exclude: #necessary to not catch the original\n",
    "\t# \t\ttemp = grouped_text.query('@pair[0] == Real_ID_y and @pair[1] == Real_ID_x')\n",
    "\t# \t\texclude.append(temp.index[0])\n",
    "\t# \t\ttemp_df = temp_df.append(temp, ignore_index = True)\n",
    "\n",
    "\t# #Switch column names\n",
    "\t# col_list = list(temp_df)\n",
    "\t# col_list[0], col_list[1] = col_list[1], col_list[0]\n",
    "\t# temp_df.columns = col_list\n",
    "\t# temp_df = temp_df[[\"Real_ID_x\", \"Real_ID_y\", \"Text\"]]\n",
    "\n",
    "\t# # # Append and groupby RealID_s\n",
    "\t# # new_grouped_text = grouped_text.append(temp_df, ignore_index = True).groupby([\"Real_ID_x\", \"Real_ID_y\"])\n",
    "\n",
    "\t# # #Produce the final DF\n",
    "\t# # grouped_final = new_grouped_text['Text'].apply(lambda x: ','.join(x.astype(str))).reset_index()\n",
    "\tgrouped_final = bow_df\n",
    "# \tgrouped_final.to_csv('Real_IDs_with_Single_Sen.tsv', sep = '\\t', index = False, header = False)\n",
    "\n",
    "\tprint \"\\n\", \"Done with Pre Bow data frame\", \"\\n\"\n",
    "\n",
    "\treturn grouped_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_bow_df(pre_bow, ints):\n",
    "\n",
    "\tprint \"Producing BOW data frame\"\n",
    "\n",
    "\tinteractions = open_interactions(ints)\n",
    "\n",
    "\tinteractions[\"item_id_a\"] = interactions[\"item_id_a\"].str[5:]\n",
    "\tinteractions[\"item_id_b\"] = interactions[\"item_id_b\"].str[5:]\n",
    "\tinteractions = interactions[[\"item_id_a\", \"item_id_b\", \"mode\"]]\n",
    "\t# desired_interaction = check_interaction(interactions, 0)\n",
    "\n",
    "\tdesired_interaction = \"binding\"\n",
    "\n",
    "\tfor i,r in pre_bow.iterrows():\n",
    "\t\tpre_mode = interactions.loc[interactions[\"item_id_a\"] == r[\"Real_ID_x\"]]\n",
    "\t\tmode = pre_mode.loc[pre_mode[\"item_id_b\"] == r[\"Real_ID_y\"], (\"mode\")]\n",
    "\t\tif mode.empty or desired_interaction not in mode.values:\n",
    "\t\t\tpre_bow.set_value(i, \"Mode\", 0)\n",
    "\t\telif desired_interaction in mode.values:\n",
    "\t\t\tpre_bow.set_value(i, \"Mode\", 1)\n",
    "        \n",
    "\tpre_bow[\"Mode\"] = pre_bow[\"Mode\"].astype(int)\n",
    "\tpre_bow = pre_bow[[\"Real_ID_x\", \"Real_ID_y\", \"Mode\", \"Text\"]]\n",
    "\n",
    "\tpre_bow.to_csv('Bag_of_Words_df_Single_Sen.tsv', sep = '\\t', index = False)\n",
    "\n",
    "\tprint \"\\n\", \"Done with Bag of Words data frame\", \"\\n\"\n",
    "\n",
    "\treturn pre_bow\n",
    "\n",
    "\n",
    "def check_interaction(interactions, tries = 0):\n",
    "\n",
    "\tdesired_interaction = raw_input(\"What interaction will be positive?: \")\n",
    "\tif desired_interaction in interactions[\"mode\"].tolist():\n",
    "\t\treturn desired_interaction\n",
    "\telif tries < 10:\n",
    "\t\tprint \"There isn't such interaction. Try again\"\n",
    "\t\ttries+=1\n",
    "\t\tcheck_interaction(interactions, tries)\n",
    "\telif tries == 10:\n",
    "\t\tprint \"You are hopeless. Please check interaction again! Defaulting to physical binding!\"\n",
    "\t\tdesired_interaction = \"binding\"\n",
    "\t\treturn desired_interaction\n",
    "\n",
    "def split_train_test(bow_df):\n",
    "\n",
    "\tdata = bow_df[[\"Real_ID_x\", \"Real_ID_y\", \"Text\"]]\n",
    "\tlabels = bow_df[\"Mode\"]\n",
    "\n",
    "\t# test_s = input(\"What is the size of test? (0-1, float): \")\n",
    "\t# ran_state = input(\"Set random state (int): \")\n",
    "\ttest_s = 0.3\n",
    "\tran_state = 1993\n",
    "\n",
    "\tdata_train = data.sample(frac=1-test_s, random_state = ran_state)\n",
    "\tdata_test = data.drop(data_train.index)\n",
    "\tlabels_train = labels.sample(frac=1-test_s, random_state = ran_state)\n",
    "\tlabels_test = labels.drop(labels_train.index)\n",
    "\n",
    "\tdata_train, data_test, labels_train, labels_test = data_train.reset_index(drop=True), data_test.reset_index(drop=True), labels_train.reset_index(drop=True), labels_test.reset_index(drop=True) \n",
    "\n",
    "\tdata_train.to_csv('Data_train_seed_' + str(ran_state) + '.tsv', sep = '\\t', index = False, header = False)\n",
    "\tdata_test.to_csv('Data_test_seed_' + str(ran_state) + '.tsv', sep = '\\t', index = False, header = False)\n",
    "\tlabels_train.to_csv('Labels_train_seed_' + str(ran_state) + '.tsv', sep = '\\t', index = False, header = False)\n",
    "\tlabels_test.to_csv('Labels_test_seed_' + str(ran_state) + '.tsv', sep = '\\t', index = False, header = False)\n",
    "\n",
    "\treturn data_train, data_test, labels_train, labels_test\n",
    "\n",
    "def texts_to_words( raw_text ):\n",
    "\t# Function to convert a raw text to a string of words\n",
    "\t# The input is a single string (a raw text), and \n",
    "\t# the output is a single string (a preprocessed text)\n",
    "\t#\n",
    "\t# 2. Remove non-letters        \n",
    "\tletters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text) \n",
    "\t#\n",
    "\t# 3. Convert to lower case, split into individual words\n",
    "\twords = letters_only.lower().split()                             \n",
    "\t#\n",
    "\t# 4. In Python, searching a set is much faster than searching\n",
    "\t#   a list, so convert the stop words to a set\n",
    "\tstops = set(stopwords.words(\"english\"))                  \n",
    "\t# \n",
    "\t# 5. Remove stop words\n",
    "\tmeaningful_words = [w for w in words if not w in stops]   \n",
    "\t#\n",
    "\t# 6. Join the words back into one string separated by space, \n",
    "\t# and return the result.\n",
    "\treturn( \" \".join( meaningful_words ))  \n",
    "\n",
    "def bag_of_words_and_prediction(bow_df):\n",
    "\n",
    "\tdata_train, data_test, labels_train, labels_test = split_train_test(bow_df)\n",
    "\n",
    "\t# Get the number of reviews based on the dataframe column size\n",
    "\tnum_texts = data_train[\"Text\"].size\n",
    "\n",
    "\t# Initialize an empty list to hold the clean reviews\n",
    "\tprint \"Cleaning and parsing the training set article sentences...\\n\"\n",
    "\tclean_train_texts = []\n",
    "\tfor i in xrange( 0, num_texts ):\n",
    "\t\t# If the index is evenly divisible by 1000, print a message\n",
    "\t\t# if( (i+1)%100 == 0 ):\n",
    "\t\t\t# print \"Texts %d of %d\\n\" % ( i+1, num_texts )                                                                    \n",
    "\t\tclean_train_texts.append( texts_to_words( data_train[\"Text\"][i] ))\n",
    "\n",
    "\tprint \"Creating the bag of words...\\n\"\n",
    "\t# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "\t# bag of words tool.  \n",
    "\tvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 1000) \n",
    "\n",
    "\t# fit_transform() does two functions: First, it fits the model\n",
    "\t# and learns the vocabulary; second, it transforms our training data\n",
    "\t# into feature vectors. The input to fit_transform should be a list of \n",
    "\t# strings.\n",
    "\ttrain_data_features = vectorizer.fit_transform(clean_train_texts)\n",
    "\n",
    "\t# Numpy arrays are easy to work with, so convert the result to an \n",
    "\t# array\n",
    "\ttrain_data_features = train_data_features.toarray()\n",
    "\n",
    "\tprint \"Training the random forest...\"\n",
    "\n",
    "\t# Initialize a Random Forest classifier with 100 trees\n",
    "\tforest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "\t# Fit the forest to the training set, using the bag of words as \n",
    "\t# features and the sentiment labels as the response variable\n",
    "\t#\n",
    "\t# This may take a few minutes to run\n",
    "\tforest = forest.fit( train_data_features, labels_train)\n",
    "\n",
    "\tnum_texts = len(data_test[\"Text\"])\n",
    "\tclean_test_texts = [] \n",
    "\n",
    "\tprint \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "\tfor i in xrange(0,num_texts):\n",
    "\t\t# if( (i+1) % 1000 == 0 ):\n",
    "\t\t\t# print \"Review %d of %d\\n\" % (i+1, num_texts)\n",
    "\t\tclean_texts = texts_to_words( data_test[\"Text\"][i] )\n",
    "\t\tclean_test_texts.append( clean_texts )\n",
    "\n",
    "\t# Get a bag of words for the test set, and convert to a numpy array\n",
    "\ttest_data_features = vectorizer.transform(clean_test_texts)\n",
    "\ttest_data_features = test_data_features.toarray()\n",
    "\n",
    "\t# Use the random forest to make sentiment label predictions\n",
    "\tprint \"Predicting based on model...\"\n",
    "\tresult = forest.predict(test_data_features)\n",
    "\n",
    "\terror = get_accuracy(result, labels_test)\n",
    "\n",
    "\treturn error\n",
    "\n",
    "def get_accuracy(l_new, l_te):\n",
    "\t\"\"\"Calculates the accuracy of predicted labels, based on the given labels\n",
    "\n",
    "\tINPUT: New(Predicted) Labels, Test Labels\n",
    "\n",
    "\tOUTPUT: Error  \"\"\"\n",
    "\n",
    "\tacc = 0\n",
    "\n",
    "\tfor i in range(len(l_te)):\n",
    "\t\tif l_new[i] == l_te[i]:\n",
    "\t\t\tacc += 1\n",
    "\n",
    "\tacc = float(acc / len(l_te))\n",
    "\n",
    "\treturn 1-acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing Pre BOW data frame\n",
      "Couldn't mask  SOS1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  inorganic pyrophosphatase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  superoxide dismutase [Cu-Zn  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  ORM1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glutamate synthase (NADH  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  superoxide dismutase [Cu-Zn  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  carbonic anhydrase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Rad27  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  malate dehydrogenase (oxaloacetate-decarboxylating  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  ribonuclease H  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  alpha-2  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  RNase H(35  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glutamine synthetase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Spc1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glutamate synthase (NADH  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  nucleotidase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glutamine synthetase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  superoxide dismutase [Cu-Zn  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  I(c  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  sos1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  SOS1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  ferredoxin NADP(+) reductase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  3-oxoacyl-[acyl-carrier-protein]-reductase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  SOS1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  CPY  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glutamate synthase (NADH  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  vacuolar Na(+)/H(+) exchanger  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  ORM1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  CaM  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+)-antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  SOS1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  carbonic-anhydrase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glutamate synthase (NADH  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  glucose-6-phosphate dehydrogenase  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  superoxide dismutase [Cu-Zn  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  Na(+)/H(+) antiporter  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  superoxide dismutase [Cu-Zn  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  SOS1  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  I(C  with RegEx\n",
      "Trying a different way!\n",
      "Couldn't mask  superoxide dismutase [Cu-Zn  with RegEx\n",
      "Trying a different way!\n"
     ]
    }
   ],
   "source": [
    "pre_bow = produce_pre_bow(pairs_nec, ents) \n",
    "\n",
    "# next_task(all_files)\n",
    "\n",
    "################# END OF PRE BOW #################\n",
    "\n",
    "################# BAG OF WORDS DF ################\n",
    "\n",
    "bow_df = produce_bow_df(pre_bow, ints)\n",
    "\n",
    "# next_task(all_files)\n",
    "\n",
    "################# END OF BAG OF WORDS DF #########\n",
    "\n",
    "############ BAG OF WORDS AND RAND FOR ###########\n",
    "\n",
    "resulting_error = bag_of_words_and_prediction(bow_df)\n",
    "\n",
    "print \"\\n\", \"The resulting error is \", resulting_error, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
